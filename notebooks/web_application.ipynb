{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "The a dash-application will be used to show-case the cluster collection after K-means clustering and RFM- analysis applied to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web-app helps users to filter through cluster infomation about clients, displaying useful information data-analysts can us to predict behavioural tendencies of \n",
    "an cluster's clients. The behavioural tendencies of the clusters are examined according to the frequency they shop from the enterprise and the total amount they spent at the company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionality\n",
    "\n",
    "- User Interface: The web application works with a dropbox wich receives a processed CSV file, then tunes the cluster collection being previed by adjusting the values at the amount and frequency sliders of customers.\n",
    "- Back-end programming: When droping a csv file into the dropbox, the dataset is preprocessed and a K-means clustering model is applied to the dataset and then the sliders adjusts the cluster collection the analyst using the web-application preceives, according to the amount and frequency.\n",
    "- The output-display:  The display will display the RMF, k-means-clustered models and will adapt changes as the amount and frequency sliders are adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dash application functionality\n",
    "\n",
    "The dash application is utilised to create interactive websites data-analysts can use to make model-predictions and showcase differences between cluster clients in our libraries. The website-application utilizes interactive packages with external CSS stylisation resources, HTML and Python scripts to make the dash-application design more intuative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-end functionality:\n",
    "\n",
    "The following libraries from the source package will prepare the dataset and apply a K-means clustering model to the dataset to produce an dataset which can be visualised by the web-application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"begin\"\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# local DataFrame\n",
    "df = pd.DataFrame()\n",
    "\"\"\"function to load the data\"\"\"\n",
    "def prepare_dataset(df):\n",
    "    \"Data Cleaning\"\n",
    "    # Drop 'Country' and 'InvoiceNo' columns\n",
    "    processed_df = df.drop(['Country','Description'], axis=1)\n",
    "    # Remove rows with quantity less than or equal to zero\n",
    "    processed_df = processed_df[processed_df['Quantity'] >= 0]\n",
    "    # Remove rows with missing CustomerID\n",
    "    processed_df = processed_df.dropna(subset=['CustomerID'])\n",
    "    # Reset the index after removing rows\n",
    "    processed_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    \"Data Processing\"\n",
    "    processed_df['Quantity'] = processed_df['Quantity'].astype(int)\n",
    "    processed_df['CustomerID'] = processed_df['CustomerID'].astype(str)\n",
    "    processed_df['Amount'] = processed_df['Quantity']*processed_df['UnitPrice']\n",
    "    # amount\n",
    "    rfm_ds_n = processed_df.groupby('CustomerID')['Amount'].sum()\n",
    "    rfm_ds_n.reset_index()\n",
    "    rfm_ds_n.columns = ['CustomerID', 'Amount']\n",
    "    # frequency\n",
    "    rfm_ds_f = processed_df.groupby('CustomerID')['InvoiceNo'].count()\n",
    "    rfm_ds_f = rfm_ds_f.reset_index()\n",
    "    rfm_ds_f.columns = ['CustomerID','Frequency']\n",
    "    # recency\n",
    "    'date_diff'\n",
    "    processed_df['InvoiceDate'] = pd.to_datetime(processed_df['InvoiceDate'],format='%m/%d/%Y %H:%M')\n",
    "    max_date = max(processed_df['InvoiceDate'])\n",
    "    processed_df['Diff'] = max_date - processed_df['InvoiceDate']\n",
    "    rfm_ds_p = processed_df.groupby('CustomerID')['Diff'].min()\n",
    "    rfm_ds_p = rfm_ds_p.reset_index()\n",
    "    rfm_ds_p.columns = ['CustomerID', 'Diff']\n",
    "    rfm_ds_p['Diff'] = rfm_ds_p['Diff'].dt.days\n",
    "    # merge\n",
    "    rfm_ds_final = pd.merge(rfm_ds_n, rfm_ds_f, on='CustomerID',how='inner')\n",
    "    rfm_ds_final = pd.merge(rfm_ds_final, rfm_ds_p, on='CustomerID', how='inner')\n",
    "    rfm_ds_final.columns = ['CustomerID', 'Amount', 'Frequency', 'Recency']\n",
    "    return rfm_ds_final\n",
    "\"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outlier data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"begin\"\n",
    "import pandas as pd\n",
    "\n",
    "# local DataFrame\n",
    "df = pd.DataFrame()\n",
    "\"\"\"function to process the data\"\"\"\n",
    "\n",
    "def process_data(df):\n",
    "        #Removing outliers\n",
    "        Q1 = df['Amount'].quantile(0.25)\n",
    "        Q3 = df['Amount'].quantile(0.75)\n",
    "        IQR = Q3-Q1\n",
    "        rfm_ds_final = df[(df['Amount'] > Q1 - 1.5*IQR) & (df['Amount'] < Q3 + 1.5*IQR)]\n",
    "\n",
    "        Q1 = df['Recency'].quantile(0.25)\n",
    "        Q3 = df['Recency'].quantile(0.75)\n",
    "        IQR = Q3-Q1\n",
    "        rfm_ds_final = df[(df['Recency'] > Q1 - 1.5*IQR) & (df['Recency'] < Q3 + 1.5*IQR)]\n",
    "\n",
    "        Q1 = df['Frequency'].quantile(0.25)\n",
    "        Q3 = df['Frequency'].quantile(0.75)\n",
    "        IQR = Q3-Q1\n",
    "        rfm_ds_final = df[(df['Frequency'] > Q1 - 1.5*IQR) & (df['Frequency'] < Q3 + 1.5*IQR)]\n",
    "        \n",
    "        #Dont need Min-max scaling\n",
    "        X = rfm_ds_final\n",
    "        return X\n",
    "\n",
    "\"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"begin\"\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\"\"\"function to create the model\"\"\"\n",
    "def getClusters(df):\n",
    "\n",
    "    #model creation\n",
    "    kmeans = KMeans(n_clusters= 3,max_iter= 50)\n",
    "    kmeans.fit(df)\n",
    "    lbs = kmeans.labels_\n",
    "\n",
    "    \"elbow-method\"\n",
    "    #appendin inertia\n",
    "    #wss\n",
    "    wss =[]\n",
    "    range_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n",
    "    for num_clusters in range_n_clusters:\n",
    "        kmeans = KMeans(n_clusters= num_clusters, max_iter= 50)\n",
    "        kmeans.fit(df)\n",
    "        wss.append(kmeans.inertia_)\n",
    "    #silhouette score\n",
    "    n_cluster=0\n",
    "    silhouette_no=0\n",
    "    range_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n",
    "    for num_clusters in range_n_clusters:\n",
    "        kmeans = KMeans(n_clusters= num_clusters, max_iter= 50)\n",
    "        kmeans.fit(df)\n",
    "        cluster_labels = kmeans.labels_\n",
    "        silhouette_avg = silhouette_score(df, cluster_labels)\n",
    "        print('For n_clusters{0}, the silhouette score is {1}'.format(num_clusters, silhouette_avg))\n",
    "        if silhouette_avg>silhouette_no:\n",
    "            silhouette_no=silhouette_avg\n",
    "            n_cluster=num_clusters\n",
    "    return n_cluster\n",
    "\n",
    "\"returns the final data-model\"\n",
    "def create_finalmodel(df):\n",
    "    # base model\n",
    "    df_scaled = df\n",
    "    # final_model labels\n",
    "    final_model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        KMeans(n_clusters=3, random_state=42)\n",
    "    )\n",
    "    # Predict class labels\n",
    "    cluster = final_model.fit_predict(df_scaled)\n",
    "\n",
    "    df_scaled['Cluster'] = cluster\n",
    "    return df_scaled\n",
    "\"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User interface design \n",
    "\n",
    "The following library will used to create a wep-application to display both the outlier K-means clustering model and the non-outlier K-means clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prepare_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprepare_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_dataset\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpreprocess_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process_data\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_finalmodel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'prepare_data'"
     ]
    }
   ],
   "source": [
    "\"begin\"\n",
    "import pandas as pd\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output\n",
    "import base64\n",
    "import io\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n",
    "from prepare_data import prepare_dataset\n",
    "from preprocess_data import process_data\n",
    "from train_models import create_finalmodel\n",
    "import dash_bootstrap_components as dbc\n",
    "\n",
    "\"palettes\"\n",
    "sns.color_palette(\"rocket\", as_cmap=True)\n",
    "\"outliers, no_outliers\"\n",
    "df = pd.DataFrame()\n",
    "\"code to train models\"\n",
    "def prepare_outlier_data(df):\n",
    "    df_temp = prepare_dataset(df)\n",
    "    return df_temp\n",
    "\n",
    "def prepare_non_outlier_data(df):\n",
    "    df_temp = process_data(df)\n",
    "    return df_temp\n",
    "\n",
    "def train_data(df):\n",
    "    df_temp = create_finalmodel(df)\n",
    "    return df_temp\n",
    "\n",
    "app = dash.Dash(external_stylesheets=[dbc.themes.LUX])\n",
    "server = app.server\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Customer Segmentation Dashboard\"),\n",
    "    dcc.Upload(\n",
    "        id='upload-data',\n",
    "        children=html.Div([\n",
    "            'Drag and Drop or ',\n",
    "            html.A('Select CSV File')\n",
    "        ]),\n",
    "        style={\n",
    "            'width': '100%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        multiple=False\n",
    "    ),\n",
    "    html.P(\"Filter based on amount\"),\n",
    "    html.Div([\n",
    "        dcc.RangeSlider(\n",
    "            id='amount-slider',\n",
    "            min=0,\n",
    "            max=10000,\n",
    "            value=[0, 10000],\n",
    "            marks={i: str(i) for i in range(0, 10001, 2000)}\n",
    "        )\n",
    "    ], style={'marginBottom': '30px'}),\n",
    "    html.P(\"Filter based on frequency\"),\n",
    "    html.Div([\n",
    "        dcc.RangeSlider(\n",
    "            id='frequency-slider',\n",
    "            min=0,\n",
    "            max=200,\n",
    "            value=[0, 200],\n",
    "            marks={i: str(i) for i in range(0, 201, 50)}\n",
    "        )\n",
    "    ], style={'marginBottom': '30px'}),\n",
    "    html.Div([\n",
    "        dcc.Graph(id='customer-segments-scatter'),\n",
    "        dcc.Graph(id='customer-segments-scatter-2')\n",
    "    ], style={'display': 'flex'}),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    [Output('customer-segments-scatter', 'figure'),\n",
    "     Output('customer-segments-scatter-2', 'figure')],\n",
    "    [Input('upload-data', 'contents'),\n",
    "     Input('amount-slider', 'value'),\n",
    "     Input('frequency-slider', 'value')]\n",
    ")\n",
    "def update_output(contents, amount_range, frequency_range):\n",
    "    if contents is not None:\n",
    "        content_type, content_string = contents.split(',')\n",
    "        decoded = base64.b64decode(content_string)\n",
    "        df = pd.read_csv(io.StringIO(decoded.decode('utf-8')))\n",
    "\n",
    "        temp_Out = prepare_outlier_data(df)\n",
    "        temp_n_Out = prepare_non_outlier_data(temp_Out)\n",
    "\n",
    "        temp_Out.to_csv('Out.csv', sep=',', index=False, header=True)\n",
    "        temp_n_Out.to_csv('No_Out.csv', sep=',', index=False, header=True)\n",
    "\n",
    "        Out = train_data(temp_Out)\n",
    "        n_Out = train_data(temp_n_Out)\n",
    "        Out.to_csv('Out.csv', sep=',', index=False, header=True)\n",
    "        n_Out.to_csv('No_Out.csv', sep=',', index=False, header=True)\n",
    "\n",
    "        # Filter data based on amount and frequency ranges\n",
    "        Out = Out[(Out['Amount'] >= amount_range[0]) & (Out['Amount'] <= amount_range[1]) &\n",
    "                  (Out['Frequency'] >= frequency_range[0]) & (Out['Frequency'] <= frequency_range[1])]\n",
    "        n_Out = n_Out[(n_Out['Amount'] >= amount_range[0]) & (n_Out['Amount'] <= amount_range[1]) &\n",
    "                      (n_Out['Frequency'] >= frequency_range[0]) & (n_Out['Frequency'] <= frequency_range[1])]\n",
    "\n",
    "        # Plot 2D scatter plot of customer segments for both graphs\n",
    "        fig = go.Figure(data=[go.Scatter(\n",
    "            x=Out['Frequency'],\n",
    "            y=Out['Amount'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=5,\n",
    "                color=Out['Cluster'],  # Color by cluster label\n",
    "                opacity=0.8,\n",
    "                colorscale='Viridis'\n",
    "            )\n",
    "        )])\n",
    "\n",
    "        fig2 = go.Figure(data=[go.Scatter(\n",
    "            x=n_Out['Frequency'],\n",
    "            y=n_Out['Amount'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=5,\n",
    "                color=n_Out['Cluster'],  # Color by cluster label\n",
    "                opacity=0.8,\n",
    "                colorscale='Viridis'\n",
    "            )\n",
    "        )])\n",
    "\n",
    "        fig.update_layout(\n",
    "            title='2D Plot of Frequency and Amount - With outliers',\n",
    "            xaxis_title='Frequency',\n",
    "            yaxis_title='Amount',\n",
    "            width=1000,\n",
    "            height=700\n",
    "        )\n",
    "\n",
    "        fig2.update_layout(\n",
    "            title='2D Plot of Frequency and Amount - Without outliers',\n",
    "            xaxis_title='Frequency',\n",
    "            yaxis_title='Amount',\n",
    "            width=1000,\n",
    "            height=700\n",
    "        )\n",
    "\n",
    "        return fig, fig2\n",
    "    else:\n",
    "        return {}, {}\n",
    "\n",
    "# Run the Dash app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n",
    "\"end\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG382_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
